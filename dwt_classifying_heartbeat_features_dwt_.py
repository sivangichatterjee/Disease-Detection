# -*- coding: utf-8 -*-
"""DWT_Classifying_heartbeat_features_DWT .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Iwe_iYDoeL54ImnLv5tWYqadSzdAKYDW

Details and Resources of the project in the doc:                                
https://docs.google.com/document/d/1F-KYQ5nRnDAUVDUrEbTFYGn7TEVsT8EByuM9xJLT4gA/edit
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings                        # To ignore any warnings
warnings.filterwarnings("ignore")
# %matplotlib inline
# %pylab inline
import os
import pandas as pd
import librosa
import librosa.display
import glob 
import matplotlib.pyplot as plt
# %config InlineBackend.figure_format = 'retina'

import seaborn as sns; sns.set()
import scipy.io as sio
# descriptive statistics
import scipy as sp
import scipy.stats as stats
import pywt

from google.colab import drive
drive.mount("/content/drive")

INPUT_DIR = '/content/drive/MyDrive/AI ML Things/University of Turku Research Internship'
'''
set_a_path=base_path+'/set_a'
set_a_metadata_path=base_path+'/set_a.csv'
set_b_path=base_path+'/set_b'
set_b_metadata_path=base_path+'/set_b.csv'
set_a_metadata = pd.read_csv(set_a_metadata_path)
'''
dataset_path = INPUT_DIR+'/set_a'
metadata = pd.read_csv(INPUT_DIR+'/set_a.csv')

SAMPLE_RATE = 16000
# seconds
MAX_SOUND_CLIP_DURATION=12

"""#Explorer data"""

set_a=pd.read_csv(INPUT_DIR+"/set_a.csv")
set_a.head()

set_a_timing=pd.read_csv(INPUT_DIR+"/set_a_timing.csv")
set_a_timing.head()

set_b=pd.read_csv(INPUT_DIR+"/set_b.csv")
set_b.head()

#merging both set-a and set-b
frames = [set_a, set_b]
train_ab=pd.concat(frames)
train_ab.describe()

#checking for duplicates
nb_classes=train_ab.label.unique()

print("Number of training examples=", train_ab.shape[0], "  Number of classes=", len(train_ab.label.unique()))
print (nb_classes)

"""Note: 'nan' indicate unclassified and unlabel test files"""

# visualize data distribution by category
category_group = train_ab.groupby(['label','dataset']).count()
plot = category_group.unstack().reindex(category_group.unstack().sum(axis=1).sort_values().index)\
          .plot(kind='bar', stacked=True, title="Number of Audio Samples per Category", figsize=(16,5))
plot.set_xlabel("Category")
plot.set_ylabel("Samples Count");

print('Min samples per category = ', min(train_ab.label.value_counts()))
print('Max samples per category = ', max(train_ab.label.value_counts()))

print('Minimum samples per category = ', min(train_ab.label.value_counts()))
print('Maximum samples per category = ', max(train_ab.label.value_counts()))

"""##Exploring each category individually"""





"""#Extracting Features of Data in Audio Domain

##Sound Feature: MFCCs
"""

# Checking an example generate mfccs from a audio file
example_file=INPUT_DIR+"/set_a/normal__201106111136.wav"
#y, sr = librosa.load(sample_file, offset=7, duration=7)
y, sr = librosa.load(example_file)
y

y.shape

"""#Loading Data"""

print("Number of training examples=", train_ab.shape[0], "  Number of classes=", len(train_ab.label.unique()))

# get audio data with a fix padding may also chop off some file
def load_file_data (folder,file_names, duration=12, sr=16000):
    input_length=sr*duration
    # function to load files and extract features
    # file_names = glob.glob(os.path.join(folder, '*.wav'))
    data = []
    for file_name in file_names:
        try:
            sound_file=folder+file_name
            print ("load file ",sound_file)
            # use kaiser_fast technique for faster extraction
            X, sr = librosa.load( sound_file, sr=sr, duration=duration,res_type='kaiser_fast') 
            dur = librosa.get_duration(y=X, sr=sr)
            # pad audio file same duration
            if (round(dur) < duration):
                print ("fixing audio lenght :", file_name)
                y = librosa.util.fix_length(X, input_length)                
            #normalized raw audio 
            # y = audio_norm(y)            
            # extract normalized mfcc feature from data
                        
        except Exception as e:
            print("Error encountered while parsing file: ", file)        
        
        data.append(y)
    return data

# load dataset-a, keep them separate for testing purpose
import os, fnmatch

A_folder=INPUT_DIR+'/set_a/'
# set-a
A_artifact_files = fnmatch.filter(os.listdir(INPUT_DIR+'/set_a'), 'artifact*.wav')
A_artifact_sounds = load_file_data(folder=A_folder,file_names=A_artifact_files, duration=MAX_SOUND_CLIP_DURATION)
A_artifact_labels = [0 for items in A_artifact_files]

arr=np.asarray(A_artifact_sounds)

arr.shape

arr

arr=np.transpose(arr)

arr.shape

waveletname = 'db1'
temp_list = [] #for Class Labels

###############################
"""Extract The Coeeficients"""
numrows = len(arr)   
numrows =48     #Number of features extracted from Discrete wavelet transform
numcols = len(arr[0]) 

Extracted_Features_tr=np.ndarray(shape=(numcols,numrows), dtype=float, order='F')

for i in range(numcols):
      coeff = pywt.wavedec(arr[:,i], waveletname, level=6)
      cA6, cD6, cD5, cD4, cD3, cD2, cD1 = coeff
      Extracted_Features_tr[i, 0] = sp.mean(abs(cD1[:]))
      Extracted_Features_tr[i, 1] = sp.mean(abs(cD2[:]))
      Extracted_Features_tr[i, 2] = sp.mean(abs(cD3[:]))
      Extracted_Features_tr[i, 3] = sp.mean(abs(cD4[:]))
      Extracted_Features_tr[i, 4] = sp.mean(abs(cD5[:]))
      Extracted_Features_tr[i, 5] = sp.mean(abs(cD6[:]))
      Extracted_Features_tr[i, 6] = sp.mean(abs(cA6[:]))

      Extracted_Features_tr[i, 7] = sp.std(cD1[:])
      Extracted_Features_tr[i, 8] = sp.std(cD2[:])
      Extracted_Features_tr[i, 9] = sp.std(cD3[:])
      Extracted_Features_tr[i, 10] = sp.std(cD4[:])
      Extracted_Features_tr[i, 11] = sp.std(cD5[:])
      Extracted_Features_tr[i, 12] = sp.std(cD6[:])
      Extracted_Features_tr[i, 13] = sp.std(cA6[:])

      Extracted_Features_tr[i, 14] = stats.skew(cD1[:])
      Extracted_Features_tr[i, 15] = stats.skew(cD2[:])
      Extracted_Features_tr[i, 16] = stats.skew(cD3[:])
      Extracted_Features_tr[i, 17] = stats.skew(cD4[:])
      Extracted_Features_tr[i, 18] = stats.skew(cD5[:])
      Extracted_Features_tr[i, 19] = stats.skew(cD6[:])
      Extracted_Features_tr[i, 20] = stats.skew(cA6[:])

      Extracted_Features_tr[i, 21] = stats.kurtosis(cD1[:])
      Extracted_Features_tr[i, 22] = stats.kurtosis(cD2[:])
      Extracted_Features_tr[i, 23] = stats.kurtosis(cD3[:])
      Extracted_Features_tr[i, 24] = stats.kurtosis(cD4[:])
      Extracted_Features_tr[i, 25] = stats.kurtosis(cD5[:])
      Extracted_Features_tr[i, 26] = stats.kurtosis(cD6[:])
      Extracted_Features_tr[i, 27] = stats.kurtosis(cA6[:])

      Extracted_Features_tr[i, 28] = sp.median(cD1[:])
      Extracted_Features_tr[i, 29] = sp.median(cD2[:])
      Extracted_Features_tr[i, 30] = sp.median(cD3[:])
      Extracted_Features_tr[i, 31] = sp.median(cD4[:])
      Extracted_Features_tr[i, 32] = sp.median(cD5[:])
      Extracted_Features_tr[i, 33] = sp.median(cD6[:])
      Extracted_Features_tr[i, 34] = sp.median(cA6[:])

      Extracted_Features_tr[i, 35] = np.sqrt(
          np.mean(cD1[:] ** 2)); 
      Extracted_Features_tr[i, 36] = np.sqrt(
          np.mean(cD2[:] ** 2));
      Extracted_Features_tr[i, 37] = np.sqrt(
          np.mean(cD3[:] ** 2));
      Extracted_Features_tr[i, 38] = np.sqrt(
          np.mean(cD4[:] ** 2));
      Extracted_Features_tr[i, 39] = np.sqrt(
          np.mean(cD5[:] ** 2));
      Extracted_Features_tr[i, 40] = np.sqrt(
          np.mean(cD6[:] ** 2));
      Extracted_Features_tr[i, 41] = np.sqrt(
          np.mean(cA6[:] ** 2));

      Extracted_Features_tr[i, 42] = sp.mean(
          abs(cD1[:])) / sp.mean(abs(cD2[:]))
      Extracted_Features_tr[i, 43] = sp.mean(
          abs(cD2[:])) / sp.mean(abs(cD3[:]))
      Extracted_Features_tr[i, 44] = sp.mean(
          abs(cD3[:])) / sp.mean(abs(cD4[:]))
      Extracted_Features_tr[i, 45] = sp.mean(
          abs(cD4[:])) / sp.mean(abs(cD5[:]))
      Extracted_Features_tr[i, 46] = sp.mean(
          abs(cD5[:])) / sp.mean(abs(cD6[:]))
      Extracted_Features_tr[i, 47] = sp.mean(
          abs(cD6[:])) / sp.mean(abs(cA6[:]))

Extracted_Features_tr.shape

Extracted_Features_tr















































